{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse, os, sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "sys.path.append('../test')\n",
    "from model import TestModel, evaluate\n",
    "from oads_access.oads_access import OADS_Access, OADSImageDataset\n",
    "sys.path.append('../..')\n",
    "from RetinalResources.retina_model import RetinaCortexModel\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "from result_manager.result_manager import ResultManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/home/niklas/projects/data/oads/mini_oads/'\n",
    "output_dir = '/home/niklas/projects/oads_access/output/'\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_manager = ResultManager(root=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = input_dir\n",
    "oads = OADS_Access(home)\n",
    "\n",
    "# get train, val, test split, using crops if specific size\n",
    "size = (200, 200)\n",
    "train_data, val_data, test_data = oads.get_train_val_test_split(use_crops=True, min_size=size, max_size=size)\n",
    "input_channels = np.array(train_data[0][0]).shape[-1]\n",
    "\n",
    "output_channels = len(oads.get_class_mapping())\n",
    "class_index_mapping = {key: index for index, key in enumerate(list(oads.get_class_mapping().keys()))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'retina_cortex'\n",
    "# model_name = 'test'\n",
    "\n",
    "if model_name == 'test':\n",
    "    # Initialize model\n",
    "    model = TestModel(input_channels=input_channels, output_channels=output_channels, input_shape=size, kernel_size=(3,3))\n",
    "elif model_name == 'retina_cortex':\n",
    "    model = RetinaCortexModel(n_retina_layers=2, n_retina_in_channels=input_channels, n_retina_out_channels=2, retina_width=32,\n",
    "                                input_shape=size, kernel_size=(9,9), n_vvs_layers=2, out_features=output_channels, vvs_width=32)\n",
    "\n",
    "if device == torch.device('cuda'):\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    fig = plt.figure( figsize=(nrow,rows) )\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # layer = 0\n",
    "# for layer in range(4):\n",
    "#     if type(model.module.layers[layer]) == nn.modules.Conv2d:\n",
    "#         filter = model.module.layers[layer].weight.cpu().data.clone()\n",
    "#     else:\n",
    "#         continue\n",
    "#     visTensor(filter, ch=0, allkernels=False)\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f'Layer {layer}')\n",
    "#     plt.ioff()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Get the custom dataset and dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "traindataset = OADSImageDataset(data=train_data, class_index_mapping=class_index_mapping, transform=transform, device=device)\n",
    "valdataset = OADSImageDataset(data=val_data, class_index_mapping=class_index_mapping, transform=transform, device=device)\n",
    "testdataset = OADSImageDataset(data=test_data, class_index_mapping=class_index_mapping, transform=transform, device=device)\n",
    "\n",
    "trainloader = DataLoader(traindataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "valloader = DataLoader(valdataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "testloader = DataLoader(testdataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.Size' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/niklas/projects/oads_access/notebooks/model_test.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/niklas/projects/oads_access/notebooks/model_test.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m result_manager\u001b[39m.\u001b[39;49msave_dataset_description(trainloader\u001b[39m=\u001b[39;49mtrainloader, valloader\u001b[39m=\u001b[39;49mvalloader, testloader\u001b[39m=\u001b[39;49mtestloader)\n",
      "File \u001b[0;32m~/projects/result_manager/result_manager/result_manager.py:32\u001b[0m, in \u001b[0;36mResultManager.save_dataset_description\u001b[0;34m(self, trainloader, valloader, testloader, overwrite)\u001b[0m\n\u001b[1;32m     28\u001b[0m dataset_decription \u001b[39m=\u001b[39m {}\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m name, _set \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m([\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m], [trainloader, valloader, testloader]):\n\u001b[1;32m     31\u001b[0m     dataset_decription[name] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mn_samples\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlen\u001b[39m(_set\u001b[39m.\u001b[39mdataset), \u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m: _set\u001b[39m.\u001b[39mbatch_size, \u001b[39m'\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m'\u001b[39m: _set\u001b[39m.\u001b[39mnum_workers,\n\u001b[0;32m---> 32\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m'\u001b[39m: _set\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtransform, \u001b[39m'\u001b[39m\u001b[39mclass_mapping\u001b[39m\u001b[39m'\u001b[39m: _set\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mclass_index_mapping, \u001b[39m'\u001b[39m\u001b[39mimage_size\u001b[39m\u001b[39m'\u001b[39m: trainloader\u001b[39m.\u001b[39;49mdataset[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\u001b[39m.\u001b[39;49mtolist()}\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_result(result \u001b[39m=\u001b[39m dataset_decription, filename\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdataset_description.yml\u001b[39m\u001b[39m'\u001b[39m, overwrite\u001b[39m=\u001b[39moverwrite)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "result_manager.save_dataset_description(trainloader=trainloader, valloader=valloader, testloader=testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 128 with accuracy 14.06%\n"
     ]
    }
   ],
   "source": [
    "res = evaluate(testloader, model, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_manager.save_result(res, filename='result_dict.yml', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_manager.save_model(model, filename='model', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 0\n",
      "[1,    20] loss: 119.018\n",
      "Got 9 / 128 with accuracy 7.03%\n",
      "Validation accuracy: 0.0703125\n",
      "[1,    40] loss: 0.027\n",
      "Got 11 / 128 with accuracy 8.59%\n",
      "Validation accuracy: 0.0859375\n",
      "[1,    60] loss: 0.026\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "Running epoch 1\n",
      "[2,    20] loss: 0.025\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[2,    40] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[2,    60] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "Running epoch 2\n",
      "[3,    20] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[3,    40] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[3,    60] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "Running epoch 3\n",
      "[4,    20] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[4,    40] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[4,    60] loss: 0.023\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "Running epoch 4\n",
      "[5,    20] loss: 0.023\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[5,    40] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[5,    60] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "Running epoch 5\n",
      "[6,    20] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[6,    40] loss: 0.023\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[6,    60] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "Running epoch 6\n",
      "[7,    20] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[7,    40] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[7,    60] loss: 0.023\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "Running epoch 7\n",
      "[8,    20] loss: 0.023\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[8,    40] loss: 0.023\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[8,    60] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "Running epoch 8\n",
      "[9,    20] loss: 0.024\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n",
      "[9,    40] loss: 0.023\n",
      "Got 37 / 128 with accuracy 28.91%\n",
      "Validation accuracy: 0.2890625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/niklas/projects/oads_access/notebooks/model_test.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/niklas/projects/oads_access/notebooks/model_test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/niklas/projects/oads_access/notebooks/model_test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/niklas/projects/oads_access/notebooks/model_test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/niklas/projects/oads_access/notebooks/model_test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/niklas/projects/oads_access/notebooks/model_test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/oads/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/oads/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(int(15)):  # loop over the dataset multiple times\n",
    "    print(f\"Running epoch {epoch}\")\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "            val_res = evaluate(valloader, model, criterion=criterion)\n",
    "            print(f\"Validation accuracy: {val_res['accuracy']}\")\n",
    "\n",
    "print(f'Finished Training with loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 25 / 128 with accuracy 19.53%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.1953125,\n",
       " 'batch_losses': {0: 2.329002618789673,\n",
       "  1: 2.189633369445801,\n",
       "  2: 2.0372684001922607,\n",
       "  3: 2.0097250938415527,\n",
       "  4: 2.1287879943847656,\n",
       "  5: 2.8553290367126465,\n",
       "  6: 2.0751585960388184,\n",
       "  7: 2.3353168964385986}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(testloader, model, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): GaussianNoise()\n",
      "  (retina_0): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
      "  (retina_relu_0): ReLU()\n",
      "  (retina_1): Conv2d(32, 2, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
      "  (retina_relu_1): ReLU()\n",
      "  (gaussian_noise_out): GaussianNoise()\n",
      ")\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.flatten.Flatten'>\n",
      "<class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "# # After training\n",
    "# for layer in range(4):\n",
    "#     if type(model.module.layers[layer]) == nn.modules.Conv2d:\n",
    "#         filter = model.module.layers[layer].weight.cpu().data.clone()\n",
    "#     else:\n",
    "#         print(type(model.module.layers[layer]))\n",
    "#         continue\n",
    "#     visTensor(filter, ch=0, allkernels=True)\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f'Layer {layer}')\n",
    "#     plt.ioff()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = []\n",
    "for name, module in model.module.named_modules():\n",
    "    if not isinstance(module, nn.Sequential):\n",
    "        if type(module) == nn.modules.conv.Conv2d or type(module) == nn.Conv2d:\n",
    "            filter = module.weight.cpu().data.clone()\n",
    "        else:\n",
    "            continue\n",
    "        fig = visTensor(filter, ch=0, allkernels=True)\n",
    "        figs.append(fig)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Layer: {name}')\n",
    "        plt.ioff()\n",
    "        # plt.show()\n",
    "\n",
    "result_manager.save_pdf(figs=figs, filename='layer_visualisation_after_training.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('oads')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35f1b51645cdcbb206d6bffd09b7967f1e22f262807a6af3fb1ff35a1a83c6cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
